{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-PLRNdN6g-h"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAyvxn4E03uB"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() == True else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8WonL1YvptrX",
    "outputId": "768e3394-90ab-4583-dc01-d3c0379c90f8"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvpyVS_WdRnY"
   },
   "outputs": [],
   "source": [
    "def new_data2(file, lines):\n",
    "\n",
    "    d = {}\n",
    "    ind2w = {0: \"SOS\", 1: \"EOS\"}\n",
    "    lets = 2\n",
    "\n",
    "    f = open(file)\n",
    "    mass = []\n",
    "    mws = []  # molec weigths\n",
    "    forms = []  # molec form\n",
    "    types_ = []  # types nan, MS1, MS2 : 0, 1, 2\n",
    "    mspec = []\n",
    "    smi = []\n",
    "\n",
    "    c = 0\n",
    "    l = 0\n",
    "    for line in f:\n",
    "        l += 1\n",
    "        if len(line.split()) != 2:\n",
    "            c = 1\n",
    "        else:\n",
    "            c = 0\n",
    "        if c == 1:\n",
    "            smi.append(line.split(\",\")[0])\n",
    "            mws.append(float(line.split(\",\")[-1]))\n",
    "\n",
    "            forms.append(np.array([int(ii) for ii in line.split(\",\")[4:10]]))\n",
    "            if line.split(\",\")[1] == \"nan\":\n",
    "                types_.append(0)\n",
    "            else:\n",
    "                if line.split(\",\")[1] == \"MS1\":\n",
    "                    types_.append(1)\n",
    "                elif line.split(\",\")[1] == \"MS2\":\n",
    "                    types_.append(2)\n",
    "\n",
    "            for letter in line.split(\",\")[0]:\n",
    "                if letter not in d:\n",
    "                    d[letter] = int(lets)\n",
    "                    ind2w[lets] = letter\n",
    "                    lets += 1\n",
    "\n",
    "        if c == 0:\n",
    "            mspec.append((float(line.split()[0]), float(line.split()[1])))\n",
    "        if c == 1 or l == lines:\n",
    "            mass.append(mspec)\n",
    "            mspec = []\n",
    "\n",
    "    return (\n",
    "        np.array(mass),\n",
    "        np.array(mws),\n",
    "        np.array(forms),\n",
    "        np.array(types_),\n",
    "        np.array(smi),\n",
    "        d,\n",
    "        ind2w,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jUfNun-KDWq7",
    "outputId": "912776eb-79c3-4a8d-e8bf-5fa1355d6440"
   },
   "outputs": [],
   "source": [
    "f = \"path_to_file\"\n",
    "num_lines = sum(1 for line in open(f))\n",
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUG7QyJiMnPY"
   },
   "outputs": [],
   "source": [
    "x, x2, x3, x4, y, dic, ind2w = new_data2(f, num_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kf7we35kko_w"
   },
   "outputs": [],
   "source": [
    "x = np.delete(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spouyFUTkdrd"
   },
   "outputs": [],
   "source": [
    "# REMOVE OUTLIER; MOLECULES WITH MASS WEIGHT > 300\n",
    "maxw = 300\n",
    "\n",
    "for wei in reversed(x2):\n",
    "    if wei > maxw:\n",
    "        idx = np.where(x2 == wei)\n",
    "\n",
    "        y = np.delete(y, idx)\n",
    "        x = np.delete(x, idx)\n",
    "        x2 = np.delete(x2, idx)\n",
    "        x3 = np.delete(x3, idx, axis=0)\n",
    "        x4 = np.delete(x4, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "IKt0z_YVlT6n",
    "outputId": "3b4e6d71-663d-42e5-e852-6279bd44e1f5"
   },
   "outputs": [],
   "source": [
    "lens = {}\n",
    "ml = 0\n",
    "for smi in y:\n",
    "    if len(smi) > ml:\n",
    "        ml = len(smi)\n",
    "    else:\n",
    "        ml = ml\n",
    "    if len(smi) not in lens:\n",
    "        lens[len(smi)] = 1\n",
    "    else:\n",
    "        lens[len(smi)] += 1\n",
    "plt.bar(lens.keys(), lens.values())\n",
    "\n",
    "print(\"max len\", ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Q7BhdkJSBN3"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    X_train1,\n",
    "    X_val1,\n",
    "    X_train2,\n",
    "    X_val2,\n",
    "    X_train3,\n",
    "    X_val3,\n",
    "    X_train4,\n",
    "    X_val4,\n",
    "    y_train,\n",
    "    y_val,\n",
    ") = train_test_split(\n",
    "    x, x2, x3, x4, y, test_size=0.1, random_state=42, shuffle=True\n",
    ")  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8g9GDr3rxm4i"
   },
   "outputs": [],
   "source": [
    "# Encoder as Set transformer classes based from juho lee https://arxiv.org/pdf/1810.00825.pdf\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "\n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "\n",
    "        A = torch.softmax(Q_.bmm(K_.transpose(1, 2)) / math.sqrt(self.dim_V), 2)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, \"ln0\", None) is None else self.ln0(O)\n",
    "        O = O + F.relu(self.fc_o(O))\n",
    "        O = O if getattr(self, \"ln1\", None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(X, X)\n",
    "\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X)\n",
    "\n",
    "\n",
    "class SetTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_input,\n",
    "        num_outputs,\n",
    "        dim_output,\n",
    "        num_inds,\n",
    "        dim_hidden,\n",
    "        num_heads,\n",
    "        ln=False,\n",
    "    ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "            SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "            SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "            nn.Linear(dim_hidden, dim_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dec(self.enc(X))\n",
    "\n",
    "\n",
    "# Decoder part\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, embed_size, hidden_size, vocab_size, num_layers, dp, max_length=ml\n",
    "    ):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_size, hidden_size + 8, num_layers, batch_first=True, dropout=dp\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(hidden_size + 8, vocab_size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, in_smiles, hidden, cell):  #\n",
    "        embeddings = self.embed(in_smiles).view(1, 1, -1)\n",
    "        embeddings = F.relu(embeddings)\n",
    "        output, (hidden, cell) = self.lstm(embeddings, (hidden, cell))\n",
    "        outputs = self.softmax(self.out(output[0]))\n",
    "        return outputs, hidden, cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "GhG6mSsj1COV",
    "outputId": "65beaa78-0178-428b-9f95-308b2d41121d"
   },
   "outputs": [],
   "source": [
    "tf = 0.75\n",
    "VOC = len(ind2w)\n",
    "VOC, ind2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtHnFfShxm5D"
   },
   "outputs": [],
   "source": [
    "def tensor_from_smiles(smiles_b):\n",
    "\n",
    "    indexes = [dic[let] for let in smiles_b]\n",
    "    indexes.append(int(1))  # 0 : EOS token\n",
    "    in_smiles = torch.LongTensor(indexes).to(device)\n",
    "\n",
    "    return in_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2S9U4jNSBY6"
   },
   "outputs": [],
   "source": [
    "def get_layers(N, HID_DIM, hidden_st):\n",
    "    # for creating inputs to the LSTM with n layer:\n",
    "    temp = torch.randn(N, 1, HID_DIM).to(device)\n",
    "    for lay in range(N):\n",
    "        temp[lay] = hidden_st\n",
    "    hidden1 = temp\n",
    "\n",
    "    return hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6a2nBKhQR3j"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder1, decoder, x, x2, x3, x4, smi, LAY, HID):\n",
    "\n",
    "    loss2 = 0\n",
    "    hidden0 = encoder1(x)\n",
    "\n",
    "    hidden2 = torch.cat((hidden0.squeeze(0), x2, x3, x4), dim=1)\n",
    "    hidden2 = get_layers(LAY, HID + 8, hidden2)\n",
    "\n",
    "    cell = hidden2\n",
    "    hidden = hidden2\n",
    "    pretarget = tensor_from_smiles(smi)\n",
    "    target = torch.LongTensor([[0]]).to(device)\n",
    "\n",
    "    for di in range(len(pretarget)):\n",
    "        output, hidden, cell = decoder(target, hidden, cell)\n",
    "        topv, topi = output.data.topk(1)\n",
    "        target = topi.squeeze().unsqueeze(dim=0).detach()\n",
    "        loss2 += criterion(output, pretarget[di].unsqueeze(dim=0))\n",
    "        if int(topi[0][0]) == 1:\n",
    "            break\n",
    "\n",
    "    voss = loss2.item() / len(pretarget)\n",
    "    return voss\n",
    "\n",
    "\n",
    "def evaval(encoder1, decoder, test_pairs, LAY, HID):\n",
    "    voss = 0\n",
    "    for i in range(len(test_pairs[0])):\n",
    "\n",
    "        X = torch.tensor(test_pairs[0][i]).unsqueeze(dim=0).to(device)\n",
    "        X2 = (\n",
    "            torch.tensor(test_pairs[1][i])\n",
    "            .unsqueeze(dim=0)\n",
    "            .unsqueeze(dim=0)\n",
    "            .float()\n",
    "            .to(device)\n",
    "        )\n",
    "        X3 = (\n",
    "            torch.tensor(np.array(test_pairs[2][i])).unsqueeze(dim=0).float().to(device)\n",
    "        )\n",
    "        X4 = (\n",
    "            torch.tensor(test_pairs[3][i])\n",
    "            .unsqueeze(dim=0)\n",
    "            .unsqueeze(dim=0)\n",
    "            .float()\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        smi = test_pairs[4][i]\n",
    "        voss += evaluate(encoder1, decoder, X, X2, X3, X4, smi, LAY, HID)\n",
    "    voss_prom = voss / (len(test_pairs[0]))\n",
    "    return voss_prom\n",
    "\n",
    "\n",
    "def evaluateR(\n",
    "    encoder1, decoder, x, x2, x3, x4, LAY, HID, max_length=ml\n",
    "    hidden0 = encoder1(x)\n",
    "\n",
    "    hidden2 = torch.cat((hidden0.squeeze(0), x2, x3, x4), dim=1)\n",
    "    hidden2 = get_layers(LAY, HID + 8, hidden2)\n",
    "\n",
    "    hidden = hidden2\n",
    "    cell = hidden2\n",
    "    target = torch.LongTensor([[0]]).to(device)  # 0: SOS\n",
    "    decoded_words = []\n",
    "    for di in range(max_length):\n",
    "        output, hidden, cell = decoder(target, hidden, cell)\n",
    "        topv, topi = output.data.topk(1)\n",
    "        if int(topi[0][0]) == int(1):  # EOS\n",
    "            decoded_words.append(\"<EOS>\")\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(ind2w[int(topi[0][0])])\n",
    "        target = topi.squeeze().unsqueeze(dim=0).detach()  ###\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateRandomly(encoder1, decoder, ppair, LAY, HID, n=50):\n",
    "    for i in range(n):\n",
    "        choice = random.randint(0, len(ppair) - 1)\n",
    "        xt = torch.tensor(ppair[0][choice]).unsqueeze(dim=0).to(device)\n",
    "        x2t = (\n",
    "            torch.tensor(ppair[1][choice])\n",
    "            .unsqueeze(dim=0)\n",
    "            .unsqueeze(dim=0)\n",
    "            .float()\n",
    "            .to(device)\n",
    "        )\n",
    "        x3t = (\n",
    "            torch.tensor(np.array(ppair[2][choice])).unsqueeze(dim=0).float().to(device)\n",
    "        )\n",
    "        x4t = (\n",
    "            torch.tensor(ppair[3][choice])\n",
    "            .unsqueeze(dim=0)\n",
    "            .unsqueeze(dim=0)\n",
    "            .float()\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        smi = ppair[4][choice]\n",
    "\n",
    "        output_words = evaluateR(encoder1, decoder, xt, x2t, x3t, x4t, LAY, HID)\n",
    "        output_s = \"\".join(output_words)\n",
    "        print(\"pred:\", output_s)\n",
    "        print(\"real:\", smi)\n",
    "\n",
    "\n",
    "def evaluateTodo(encoder1, decoder, ppair, LAY, HID):\n",
    "    for i in range(len(ppair[0])):\n",
    "        X = torch.tensor(ppair[0][i]).unsqueeze(dim=0).to(device)\n",
    "        X2 = (\n",
    "            torch.tensor(ppair[1][i])\n",
    "            .unsqueeze(dim=0)\n",
    "            .unsqueeze(dim=0)\n",
    "            .float()\n",
    "            .to(device)\n",
    "        )\n",
    "        X3 = torch.tensor(np.array(ppair[2][i])).unsqueeze(dim=0).float().to(device)\n",
    "        X4 = (\n",
    "            torch.tensor(ppair[3][i])\n",
    "            .unsqueeze(dim=0)\n",
    "            .unsqueeze(dim=0)\n",
    "            .float()\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        smi = ppair[4][i]\n",
    "        output_words = evaluateR(encoder1, decoder, X, X2, X3, X4, LAY, HID)\n",
    "        output_s = \"\".join(output_words)\n",
    "        print(\"pred:\", output_s)\n",
    "        print(\"real:\", smi)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoRTwz3D8I9J"
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TSpXA3TVRzkg"
   },
   "outputs": [],
   "source": [
    "def model_function():\n",
    "\n",
    "    EMB = 256\n",
    "    HID = 256\n",
    "    LAY = 2\n",
    "    enc_UNITS = 32\n",
    "    INDS = 32\n",
    "    HEADS = 4\n",
    "    DP_dec = 0.1\n",
    "    l_r = 1e-4\n",
    "\n",
    "    decoder = DecoderRNN(EMB, HID, VOC, LAY, DP_dec).to(device)\n",
    "    encoder1 = SetTransformer(2, 1, HID, INDS, enc_UNITS, HEADS).to(device)\n",
    "\n",
    "    epochs = N_EPOCHS\n",
    "    encoder_optimizer1 = optim.AdamW(encoder1.parameters(), lr=l_r)\n",
    "\n",
    "    decoder_optimizer = optim.AdamW(decoder.parameters(), lr=l_r)  ##\n",
    "\n",
    "    x_tpair = [X_train1, X_train2, X_train3, X_train4, y_train]\n",
    "    test_pairs = [X_val1, X_val2, X_val3, X_val4, y_val]\n",
    "    b_size = len(X_train1)\n",
    "    px = []  # plot train loss\n",
    "    py = []\n",
    "    ppx = []  # plot val loss\n",
    "    ppy = []\n",
    "    p = 0  # for print every 500 iterations (see below)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        encoder1.train()\n",
    "\n",
    "        decoder.train()\n",
    "        sum_loss = 0\n",
    "\n",
    "        for b in range(b_size):\n",
    "            p += 1\n",
    "\n",
    "            decoder_optimizer.zero_grad()\n",
    "            encoder_optimizer1.zero_grad()\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            smis = str(y_train[b])\n",
    "            X_train11 = torch.tensor(X_train1[b]).unsqueeze(dim=0).to(device)\n",
    "            X_train22 = (\n",
    "                torch.tensor(X_train2[b])\n",
    "                .unsqueeze(dim=0)\n",
    "                .unsqueeze(dim=0)\n",
    "                .float()\n",
    "                .to(device)\n",
    "            )\n",
    "            X_train33 = (\n",
    "                torch.tensor(np.array(X_train3[b])).unsqueeze(dim=0).float().to(device)\n",
    "            )\n",
    "            X_train44 = (\n",
    "                torch.tensor(X_train4[b])\n",
    "                .unsqueeze(dim=0)\n",
    "                .unsqueeze(dim=0)\n",
    "                .float()\n",
    "                .to(device)\n",
    "            )\n",
    "\n",
    "            pretarget = tensor_from_smiles(smis)\n",
    "\n",
    "            hidden0 = encoder1(X_train11)  # inithidden , features from encoder output\n",
    "\n",
    "            hidden2 = torch.cat(\n",
    "                (hidden0.squeeze(0), X_train22, X_train33, X_train44), dim=1\n",
    "            )\n",
    "            hidden2 = get_layers(LAY, HID + 8, hidden2)\n",
    "\n",
    "            hidden = hidden2\n",
    "            cell = hidden2\n",
    "            target = torch.LongTensor([0]).to(device)  # 0: SOS\n",
    "            use_tf = True if random.random() < tf else False\n",
    "            if use_tf:\n",
    "                for s in range(len(pretarget)):\n",
    "                    output, hidden, cell = decoder(target, hidden, cell)\n",
    "                    target = pretarget[s].unsqueeze(dim=0)\n",
    "\n",
    "                    loss += criterion(output, target)\n",
    "            else:\n",
    "                for s in range(len(pretarget)):\n",
    "                    output, hidden, cell = decoder(target, hidden, cell)\n",
    "                    topv, topi = output.data.topk(1)\n",
    "                    target = topi.squeeze().unsqueeze(dim=0).detach()\n",
    "                    loss += criterion(output, pretarget[s].unsqueeze(dim=0))\n",
    "\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item() / len(pretarget)\n",
    "\n",
    "            encoder_optimizer1.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            if p % 500 == 0:\n",
    "                print(\"train loss: \", loss.item() / len(pretarget), \"iteration: \", p)\n",
    "\n",
    "        px.append(epoch)\n",
    "\n",
    "        py.append(sum_loss / b_size)\n",
    "        print(\"epoch: \", epoch, \"train loss: \", sum_loss / b_size)\n",
    "\n",
    "        encoder1.eval()\n",
    "\n",
    "        decoder.eval()\n",
    "\n",
    "        voss = evaval(encoder1, decoder, test_pairs, LAY, HID)\n",
    "        print(\"epoch: \", epoch, \"test loss: \", voss)\n",
    "        ppx.append(epoch)\n",
    "        ppy.append(voss)\n",
    "        duration = time.time() - start_time\n",
    "        print(\"duration: \", duration)\n",
    "        plt.plot(px, py, label=\"Train Loss\")\n",
    "        plt.plot(ppx, ppy, label=\"Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "        torch.save(encoder1.state_dict(), \"path_to_savefile\" + str(epoch) + \"_w\")\n",
    "        torch.save(decoder.state_dict(), \"path_to_savefile\" + str(epoch) + \"_w\")\n",
    "        torch.save(\n",
    "            encoder_optimizer1.state_dict(), \"path_to_savefile\" + str(epoch) + \"_w\"\n",
    "        )\n",
    "        torch.save(\n",
    "            decoder_optimizer.state_dict(), \"path_to_savefile\" + str(epoch) + \"_w\"\n",
    "        )\n",
    "        if epoch % 5 == 0:  # evaluate smiles every N epochs and save model\n",
    "\n",
    "            print(\"train eval: \")\n",
    "            evaluateRandomly(encoder1, decoder, x_tpair, LAY, HID)\n",
    "\n",
    "            print(\"test eval: \")\n",
    "            evaluateTodo(encoder1, decoder, test_pairs, LAY, HID)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BiG11zmPCmdv",
    "outputId": "b19eedeb-1e83-4014-cf31-86ea0ddc7eb4"
   },
   "outputs": [],
   "source": [
    "model_function()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "teachf_ESIarch_settrans.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
